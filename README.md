Durante a formação em Apache Airflow, aprendi a instalar, configurar e utilizar a ferramenta para criar, programar e monitorar pipelines de dados de forma programática.
Compreendi o funcionamento das DAGs (Directed Acyclic Graphs) e como utilizá-las para orquestrar fluxos de trabalho, controlando dependências, agendamentos e condições de execução.

Desenvolvi habilidades para implementar processos completos de ETL (Extração, Transformação e Carga), integrando diferentes fontes de dados e aplicando tratamentos antes do carregamento.
Também aprofundei meus conhecimentos sobre Data Lakes, entendendo suas camadas e melhores práticas para organização e gerenciamento dos dados.

Explorei o funcionamento dos diferentes executores do Airflow e como configurar a ferramenta para rodar em diferentes ambientes, incluindo a orquestração de pipelines no Kubernetes para maior escalabilidade e resiliência.

Ao final, obtive uma visão completa sobre como utilizar o Apache Airflow como uma ferramenta essencial na Engenharia de Dados, aplicando Python para automatizar e monitorar processos de forma robusta e eficiente.
