Durante a formação em Apache Airflow, aprendi a instalar, configurar e utilizar a ferramenta para criar, programar e monitorar pipelines de dados de forma programática. Compreendi o funcionamento das DAGs (Directed Acyclic Graphs) e como utilizá-las para orquestrar fluxos de trabalho, controlando dependências, agendamentos e condições de execução.

Desenvolvi habilidades para implementar processos completos de ETL (Extração, Transformação e Carga), integrando diferentes fontes de dados e aplicando tratamentos antes do carregamento. Também aprofundei meus conhecimentos sobre Data Lakes, entendendo suas camadas e melhores práticas para organização e gerenciamento dos dados.

Durante as práticas, utilizei uma API fake simulando a do Twitter como fonte de dados, o que permitiu trabalhar com cenários reais de integração, extração e tratamento de informações em pipelines automatizados.

Explorei o funcionamento dos diferentes executores do Airflow e como configurar a ferramenta para rodar em diferentes ambientes, incluindo a orquestração de pipelines no Kubernetes para maior escalabilidade e resiliência.

Ao final, obtive uma visão completa sobre como utilizar o Apache Airflow como uma ferramenta essencial na Engenharia de Dados, aplicando Python para automatizar e monitorar processos de forma robusta e eficiente.


Representação gráfica do fluxo do projeto
<img width="1867" height="946" alt="image" src="https://github.com/user-attachments/assets/79970292-129c-4252-8618-dd804ce073e9" />
